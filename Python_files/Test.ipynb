{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c352642e",
   "metadata": {
    "id": "c352642e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_200_lines(csv_files):\n",
    "  df = pd.read_csv(csv_file,index_col=False,names=[\"Lnuc\",\"X\",\"StrengthDrop\",\"StressDrop\",\"Front\",\"Slip\",\"Logic\",\"StrengthwithNuc\"])\n",
    "  df = df.iloc[::2,:]\n",
    "  return df\n",
    "\n",
    "def read_100_lines(csv_files):\n",
    "  df = pd.read_csv(csv_file,index_col=False,names=[\"Lnuc\",\"X\",\"StrengthDrop\",\"StressDrop\",\"Front\",\"Slip\",\"Logic\",\"StrengthwithNuc\"])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "NWzxPrnp5Gwa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWzxPrnp5Gwa",
    "outputId": "870d85ba-44a9-4638-d837-5a62c0095a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lnuc maximum: -inf   minimum: inf\n",
      "X maximum: -inf   minimum: inf\n",
      "StrengthDrop maximum: -inf   minimum: inf\n",
      "StressDrop maximum: -inf   minimum: inf\n",
      "Front maximum: -inf   minimum: inf\n",
      "Slip maximum: -inf   minimum: inf\n",
      "Logic maximum: -inf   minimum: inf\n",
      "StrengthwithNuc maximum: -inf   minimum: inf\n",
      "RuptureEnergy maximum: -inf   minimum: inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Print Maximum and Minimum\n",
    "max_Lnuc, min_Lnuc = -np.Inf,np.Inf\n",
    "max_X, min_X = -np.Inf, np.Inf\n",
    "max_StrengthDrop, min_StrengthDrop = -np.Inf, np.Inf\n",
    "max_StressDrop, min_StressDrop = -np.Inf, np.Inf\n",
    "max_Front, min_Front= -np.Inf, np.Inf\n",
    "max_Slip, min_Slip = -np.Inf, np.Inf\n",
    "max_Logic, min_Logic = -np.Inf,np.Inf\n",
    "max_StrengthwithNuc, min_StrengthwithNuc = -np.Inf, np.Inf\n",
    "max_RuptureEnergy, min_RuptureEnergy = -np.Inf, np.Inf\n",
    "\n",
    "print(\"Lnuc maximum: \"+str(max_Lnuc)+\"   minimum: \"+str(min_Lnuc))\n",
    "print(\"X maximum: \"+str(max_X)+\"   minimum: \"+str(min_X))\n",
    "print(\"StrengthDrop maximum: \"+str(max_StrengthDrop)+\"   minimum: \"+str(min_StrengthDrop))\n",
    "print(\"StressDrop maximum: \"+str(max_StressDrop)+\"   minimum: \"+str(min_StressDrop))\n",
    "print(\"Front maximum: \"+str(max_Front)+\"   minimum: \"+str(min_Front))\n",
    "print(\"Slip maximum: \"+str(max_Slip)+\"   minimum: \"+str(min_Slip))\n",
    "print(\"Logic maximum: \"+str(max_Logic)+\"   minimum: \"+str(min_Logic))\n",
    "print(\"StrengthwithNuc maximum: \"+str(max_StrengthwithNuc)+\"   minimum: \"+str(min_StrengthwithNuc))\n",
    "print(\"RuptureEnergy maximum: \"+str(max_RuptureEnergy)+\"   minimum: \"+str(min_RuptureEnergy))\n",
    "\n",
    "mu = 32.04   #shear elasticity\n",
    "\n",
    "\n",
    "column_max = [max_Lnuc, max_X, max_StrengthDrop, max_StressDrop, max_Front, max_Slip, max_Logic, max_StrengthwithNuc, max_RuptureEnergy]\n",
    "column_min = [min_Lnuc, min_X, min_StrengthDrop, min_StressDrop, min_Front, min_Slip, min_Logic, min_StrengthwithNuc, min_RuptureEnergy]\n",
    "# scale range below\n",
    "column_max = [16.239269, 97.6, 125.064626, 66.054391, 59.647999, 86.057297, 1, 102.996661, 20855.37742025637]\n",
    "# column_max[8] = max(0.5 * Lnuc * StrengthDrop ^ 2)\n",
    "column_min = [0.003045, -44.8, 4.100412, -10.37305, 0.0, 0.0, 0, -10.37305, 0.7013907496639992]\n",
    "column_max[8] = column_max[8] / (1.158 * mu)\n",
    "#now column_max[8] = 0.5 * Lnuc * StrengthDrop ^ 2 / (1.158mu) = fracture energy\n",
    "column_min[8] = column_min[8] / (1.158 * mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94wU2xh_Qat7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94wU2xh_Qat7",
    "outputId": "02abbb9b-38e9-40bc-e639-6e575c18b0cf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/TESTdata/Binlabel79.csv\n",
      "(43700, 9)\n",
      "E:/TESTdata/Binlabel91.csv\n",
      "(54400, 9)\n",
      "E:/TESTdata/Binlabel109.csv\n",
      "(25000, 9)\n",
      "E:/TESTdata/Binlabel119.csv\n",
      "(62300, 9)\n",
      "E:/TESTdata/Binlabel159.csv\n",
      "(82800, 9)\n",
      "E:/TESTdata/Binlabel184.csv\n",
      "(82800, 9)\n",
      "E:/TESTdata/Binlabel185.csv\n",
      "(80400, 9)\n",
      "E:/TESTdata/Binlabel186.csv\n",
      "(77700, 9)\n",
      "E:/TESTdata/Binlabel187.csv\n",
      "(83700, 9)\n",
      "E:/TESTdata/Binlabel188.csv\n",
      "(79400, 9)\n",
      "E:/TESTdata/Binlabel189.csv\n",
      "(82800, 9)\n",
      "Global fracture num = 0\n",
      "Partial fracture num = 7550\n",
      "(755000, 9)\n",
      "       Lnuc          X  StrengthDrop  StressDrop  Front      Slip  Logic  \\\n",
      "0  3.388855 -18.822162     35.042084   24.879153  5.848  1.886601    1.0   \n",
      "1  3.388855 -18.352395     35.042084   24.879153  5.760  5.421555    1.0   \n",
      "2  3.388855 -17.882628     35.042084   24.879153  5.664  7.365342    1.0   \n",
      "3  3.388855 -17.412861     35.042084   24.879153  5.600  8.575044    1.0   \n",
      "4  3.388855 -16.943095     35.042084   24.879153  5.504  9.843087    1.0   \n",
      "\n",
      "   StrengthwithNuc  RuptureEnergy  \n",
      "0        24.879153    2080.668269  \n",
      "1        24.879153    2080.668269  \n",
      "2        24.879153    2080.668269  \n",
      "3        24.879153    2080.668269  \n",
      "4        24.879153    2080.668269  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_fracture = 0\n",
    "partial_fracture = 0\n",
    "# The following annotated files are test files\n",
    "csv_files_200lines = [\n",
    "    \"./datasets/Ainlabel0.csv\",\n",
    "    \"./datasets/Ainlabel1.csv\",\n",
    "    # \"./datasets/Ainlabel2.csv\",\n",
    "    \"./datasets/Ainlabel3.csv\",\n",
    "    \"./datasets/Ainlabel68.csv\",\n",
    "    \"./datasets/Ainlabel69.csv\",\n",
    "    # the above files are homogeneous rupture  A0-A3 A68-69(except A2 for testing)\n",
    "    # all of the above samples are global rupture\n",
    "\n",
    "\n",
    "    \"./datasets/Ainlabel45.csv\",\n",
    "    \"./datasets/Ainlabel46.csv\",\n",
    "    \"./datasets/Ainlabel47.csv\",\n",
    "    \"./datasets/Ainlabel48.csv\",\n",
    "    # \"./datasets/Ainlabel49.csv\",\n",
    "    # \"./datasets/Ainlabel50.csv\",\n",
    "    # \"./datasets/Ainlabel51.csv\",\n",
    "    \"./datasets/Ainlabel52.csv\",\n",
    "    \"./datasets/Ainlabel53.csv\",\n",
    "    \"./datasets/Ainlabel54.csv\",\n",
    "    \"./datasets/Ainlabel55.csv\",\n",
    "    \"./datasets/Ainlabel56.csv\",\n",
    "    \"./datasets/Ainlabel57.csv\",\n",
    "    \"./datasets/Ainlabel58.csv\",\n",
    "    \"./datasets/Ainlabel59.csv\",\n",
    "    \"./datasets/Ainlabel60.csv\",\n",
    "    \"./datasets/Ainlabel61.csv\",\n",
    "    \"./datasets/Ainlabel64.csv\",\n",
    "    \"./datasets/Ainlabel65.csv\",\n",
    "    # \"./datasets/Ainlabel66.csv\",\n",
    "    # \"./datasets/Ainlabel67.csv\",\n",
    "    \"./datasets/Ainlabel68.csv\",\n",
    "    \"./datasets/Ainlabel69.csv\",\n",
    "    # the above files are One-Asperity \"global rupture\"\n",
    "    # A 49-51, 66-67 for testing\n",
    "\n",
    "\n",
    "   \n",
    "    # \"./datasets/Ainlabel79.csv\",\n",
    "    \n",
    "    # \"./datasets/Ainlabel91.csv\",\n",
    "    \n",
    "    # \"./datasets/Ainlabel109.csv\",\n",
    "    \n",
    "    # \"./datasets/Ainlabel119.csv\",\n",
    "    \n",
    "    \"./datasets/Ainlabel143.csv\",\n",
    "    \"./datasets/Ainlabel144.csv\",\n",
    "    \"./datasets/Ainlabel145.csv\",\n",
    "    \"./datasets/Ainlabel146.csv\",\n",
    "    \"./datasets/Ainlabel147.csv\",\n",
    "    \"./datasets/Ainlabel148.csv\",\n",
    "    \"./datasets/Ainlabel149.csv\",\n",
    "    \"./datasets/Ainlabel150.csv\",\n",
    "    \"./datasets/Ainlabel151.csv\",\n",
    "    \"./datasets/Ainlabel152.csv\",\n",
    "    \"./datasets/Ainlabel153.csv\",\n",
    "    \"./datasets/Ainlabel154.csv\",\n",
    "    \"./datasets/Ainlabel155.csv\",\n",
    "    \"./datasets/Ainlabel156.csv\",\n",
    "    \"./datasets/Ainlabel157.csv\",\n",
    "    \"./datasets/Ainlabel158.csv\",\n",
    "    \"./datasets/Ainlabel159.csv\",\n",
    "    \"./datasets/Ainlabel160.csv\",\n",
    "    \"./datasets/Ainlabel161.csv\",\n",
    "    \"./datasets/Ainlabel162.csv\",\n",
    "    \"./datasets/Ainlabel163.csv\",\n",
    "    \"./datasets/Ainlabel164.csv\",\n",
    "    \"./datasets/Ainlabel165.csv\",\n",
    "    \"./datasets/Ainlabel166.csv\",\n",
    "    \"./datasets/Ainlabel167.csv\",\n",
    "    \"./datasets/Ainlabel168.csv\",\n",
    "    \"./datasets/Ainlabel169.csv\",\n",
    "    \"./datasets/Ainlabel170.csv\",\n",
    "    \"./datasets/Ainlabel171.csv\",\n",
    "    \"./datasets/Ainlabel172.csv\",\n",
    "    \"./datasets/Ainlabel173.csv\",\n",
    "    \"./datasets/Ainlabel174.csv\",\n",
    "    \"./datasets/Ainlabel175.csv\",\n",
    "    \"./datasets/Ainlabel176.csv\",\n",
    "    \"./datasets/Ainlabel177.csv\",\n",
    "    \"./datasets/Ainlabel178.csv\",\n",
    "    \"./datasets/Ainlabel179.csv\",\n",
    "    \"./datasets/Ainlabel180.csv\",\n",
    "    \"./datasets/Ainlabel181.csv\",\n",
    "    \"./datasets/Ainlabel182.csv\",\n",
    "    \"./datasets/Ainlabel183.csv\",\n",
    "    # \"./datasets/Ainlabel184.csv\",\n",
    "    # the above files are Two-Asperity \"global rupture\"\n",
    "    # A 49-51, 66-67, 79, 91, 109, 119, 184, and 185-189 for testing\n",
    "\n",
    "    \"./datasets/Binlabel15.csv\",\n",
    "    \"./datasets/Binlabel16.csv\",\n",
    "    \"./datasets/Binlabel17.csv\",\n",
    "    \"./datasets/Binlabel18.csv\",\n",
    "    \"./datasets/Binlabel24.csv\",\n",
    "    \"./datasets/Binlabel25.csv\",\n",
    "    \"./datasets/Binlabel26.csv\",\n",
    "    \"./datasets/Binlabel27.csv\",\n",
    "    \"./datasets/Binlabel28.csv\",\n",
    "    \"./datasets/Binlabel29.csv\",\n",
    "    \"./datasets/Binlabel30.csv\",\n",
    "    \"./datasets/Binlabel31.csv\",\n",
    "    \"./datasets/Binlabel32.csv\",\n",
    "    \"./datasets/Binlabel33.csv\",\n",
    "    \"./datasets/Binlabel34.csv\",\n",
    "    \"./datasets/Binlabel35.csv\",\n",
    "    \"./datasets/Binlabel36.csv\",\n",
    "    \"./datasets/Binlabel37.csv\",\n",
    "    \"./datasets/Binlabel38.csv\",\n",
    "    \"./datasets/Binlabel39.csv\",\n",
    "    \"./datasets/Binlabel40.csv\",\n",
    "    \"./datasets/Binlabel41.csv\",\n",
    "    \"./datasets/Binlabel42.csv\",\n",
    "    \"./datasets/Binlabel43.csv\",\n",
    "    \"./datasets/Binlabel44.csv\",\n",
    "    \"./datasets/Binlabel45.csv\",\n",
    "    \"./datasets/Binlabel46.csv\",\n",
    "    \"./datasets/Binlabel47.csv\",\n",
    "    \"./datasets/Binlabel48.csv\",\n",
    "    # \"./datasets/Binlabel49.csv\",\n",
    "    # \"./datasets/Binlabel50.csv\",\n",
    "    # \"./datasets/Binlabel51.csv\",\n",
    "    \"./datasets/Binlabel52.csv\",\n",
    "    \"./datasets/Binlabel53.csv\",\n",
    "    \"./datasets/Binlabel54.csv\",\n",
    "    \"./datasets/Binlabel55.csv\",\n",
    "    \"./datasets/Binlabel56.csv\",\n",
    "    \"./datasets/Binlabel57.csv\",\n",
    "    \"./datasets/Binlabel58.csv\",\n",
    "    \"./datasets/Binlabel59.csv\",\n",
    "    \"./datasets/Binlabel60.csv\",\n",
    "    \"./datasets/Binlabel61.csv\",\n",
    "    \"./datasets/Binlabel62.csv\",\n",
    "    \"./datasets/Binlabel63.csv\",\n",
    "    \"./datasets/Binlabel64.csv\",\n",
    "    \"./datasets/Binlabel65.csv\",\n",
    "    # \"./datasets/Binlabel66.csv\",\n",
    "    # \"./datasets/Binlabel67.csv\",\n",
    "    # the above files are One-Asperity \"imcomplete rupture\"\n",
    "    # B 49-51, 66-67 for testing\n",
    "\n",
    "\n",
    "    \"./datasets/Binlabel72.csv\",\n",
    "    \"./datasets/Binlabel73.csv\",\n",
    "    \"./datasets/Binlabel74.csv\",\n",
    "    \"./datasets/Binlabel75.csv\",\n",
    "    \"./datasets/Binlabel76.csv\",\n",
    "    \"./datasets/Binlabel77.csv\",\n",
    "    \"./datasets/Binlabel78.csv\",\n",
    "    # \"./datasets/Binlabel79.csv\",\n",
    "    \"./datasets/Binlabel80.csv\",\n",
    "    \"./datasets/Binlabel81.csv\",\n",
    "    \"./datasets/Binlabel82.csv\",\n",
    "    \"./datasets/Binlabel83.csv\",\n",
    "    \"./datasets/Binlabel84.csv\",\n",
    "    \"./datasets/Binlabel85.csv\",\n",
    "    \"./datasets/Binlabel86.csv\",\n",
    "    \"./datasets/Binlabel87.csv\",\n",
    "    \"./datasets/Binlabel88.csv\",\n",
    "    \"./datasets/Binlabel89.csv\",\n",
    "    \"./datasets/Binlabel90.csv\",\n",
    "    # \"./datasets/Binlabel91.csv\",\n",
    "    \"./datasets/Binlabel92.csv\",\n",
    "    \"./datasets/Binlabel93.csv\",\n",
    "    \"./datasets/Binlabel94.csv\",\n",
    "    \"./datasets/Binlabel96.csv\",\n",
    "    \"./datasets/Binlabel100.csv\",\n",
    "    \"./datasets/Binlabel101.csv\",\n",
    "    \"./datasets/Binlabel102.csv\",\n",
    "    \"./datasets/Binlabel103.csv\",\n",
    "    \"./datasets/Binlabel104.csv\",\n",
    "    \"./datasets/Binlabel105.csv\",\n",
    "    \"./datasets/Binlabel106.csv\",\n",
    "    \"./datasets/Binlabel107.csv\",\n",
    "    \"./datasets/Binlabel108.csv\",\n",
    "    # \"./datasets/Binlabel109.csv\",\n",
    "    \"./datasets/Binlabel110.csv\",\n",
    "    \"./datasets/Binlabel111.csv\",\n",
    "    \"./datasets/Binlabel112.csv\",\n",
    "    \"./datasets/Binlabel113.csv\",\n",
    "    \"./datasets/Binlabel114.csv\",\n",
    "    \"./datasets/Binlabel115.csv\",\n",
    "    \"./datasets/Binlabel116.csv\",\n",
    "    \"./datasets/Binlabel117.csv\",\n",
    "    \"./datasets/Binlabel118.csv\",\n",
    "    # \"./datasets/Binlabel119.csv\",\n",
    "    \"./datasets/Binlabel120.csv\",\n",
    "    \"./datasets/Binlabel121.csv\",\n",
    "    \"./datasets/Binlabel122.csv\",\n",
    "    \"./datasets/Binlabel123.csv\",\n",
    "    \"./datasets/Binlabel124.csv\",\n",
    "    \"./datasets/Binlabel125.csv\",\n",
    "    \"./datasets/Binlabel126.csv\",\n",
    "    \"./datasets/Binlabel127.csv\",\n",
    "    \"./datasets/Binlabel128.csv\",\n",
    "    \"./datasets/Binlabel129.csv\",\n",
    "    \"./datasets/Binlabel130.csv\",\n",
    "    \"./datasets/Binlabel131.csv\",\n",
    "    \"./datasets/Binlabel132.csv\",\n",
    "    \"./datasets/Binlabel133.csv\",\n",
    "    \"./datasets/Binlabel134.csv\",\n",
    "    \"./datasets/Binlabel135.csv\",\n",
    "    \"./datasets/Binlabel136.csv\",\n",
    "    \"./datasets/Binlabel137.csv\",\n",
    "    \"./datasets/Binlabel138.csv\",\n",
    "    \"./datasets/Binlabel139.csv\",\n",
    "    \"./datasets/Binlabel140.csv\",\n",
    "    \"./datasets/Binlabel141.csv\",\n",
    "    \"./datasets/Binlabel142.csv\",\n",
    "    \"./datasets/Binlabel143.csv\",\n",
    "    \"./datasets/Binlabel144.csv\",\n",
    "    \"./datasets/Binlabel145.csv\",\n",
    "    \"./datasets/Binlabel146.csv\",\n",
    "    \"./datasets/Binlabel147.csv\",\n",
    "    \"./datasets/Binlabel148.csv\",\n",
    "    \"./datasets/Binlabel149.csv\",\n",
    "    \"./datasets/Binlabel150.csv\",\n",
    "    \"./datasets/Binlabel151.csv\",\n",
    "    \"./datasets/Binlabel152.csv\",\n",
    "    \"./datasets/Binlabel153.csv\",\n",
    "    \"./datasets/Binlabel154.csv\",\n",
    "    \"./datasets/Binlabel155.csv\",\n",
    "    \"./datasets/Binlabel156.csv\",\n",
    "    \"./datasets/Binlabel157.csv\",\n",
    "    \"./datasets/Binlabel158.csv\",\n",
    "    # \"./datasets/Binlabel159.csv\",\n",
    "    \"./datasets/Binlabel160.csv\",\n",
    "    \"./datasets/Binlabel161.csv\",\n",
    "    \"./datasets/Binlabel162.csv\",\n",
    "    \"./datasets/Binlabel163.csv\",\n",
    "    \"./datasets/Binlabel164.csv\",\n",
    "    \"./datasets/Binlabel165.csv\",\n",
    "    \"./datasets/Binlabel166.csv\",\n",
    "    \"./datasets/Binlabel167.csv\",\n",
    "    \"./datasets/Binlabel168.csv\",\n",
    "    \"./datasets/Binlabel169.csv\",\n",
    "    \"./datasets/Binlabel170.csv\",\n",
    "    \"./datasets/Binlabel171.csv\",\n",
    "    \"./datasets/Binlabel172.csv\",\n",
    "    \"./datasets/Binlabel173.csv\",\n",
    "    \"./datasets/Binlabel174.csv\",\n",
    "    \"./datasets/Binlabel175.csv\",\n",
    "    \"./datasets/Binlabel176.csv\",\n",
    "    \"./datasets/Binlabel177.csv\",\n",
    "    \"./datasets/Binlabel178.csv\",\n",
    "    \"./datasets/Binlabel179.csv\",\n",
    "    \"./datasets/Binlabel180.csv\",\n",
    "    \"./datasets/Binlabel181.csv\",\n",
    "    \"./datasets/Binlabel182.csv\",\n",
    "    \"./datasets/Binlabel183.csv\",\n",
    "    # \"./datasets/Binlabel184.csv\",\n",
    "    \"./datasets/Binlabel185.csv\",\n",
    "    \"./datasets/Binlabel186.csv\",\n",
    "    \"./datasets/Binlabel187.csv\",\n",
    "    \"./datasets/Binlabel188.csv\",\n",
    "    \"./datasets/Binlabel189.csv\",\n",
    "    # the above files are Two-Asperity \"imcomplete rupture\"\n",
    "    # B66-67, 79, 91, 109, 119, 184, and 185-189 for testing\n",
    "]\n",
    "\n",
    "csv_files_100lines = [\n",
    "    # \"./datasets/inlabel30.csv\",\n",
    "    # \"./datasets/inlabel31.csv\",\n",
    "    # \"./datasets/inlabel32.csv\",\n",
    "    # \"./datasets/inlabel33.csv\",\n",
    "    # \"./datasets/inlabel34.csv\",\n",
    "    # \"./datasets/inlabel35.csv\",\n",
    "    # \"./datasets/inlabel36.csv\",\n",
    "    # \"./datasets/inlabel37.csv\",\n",
    "    # \"./datasets/inlabel38.csv\",\n",
    "    # \"./datasets/inlabel39.csv\",\n",
    "\n",
    "]\n",
    "# Read all csv files for training\n",
    "data_frames = []\n",
    "for csv_file in csv_files_200lines:\n",
    "    print(csv_file)\n",
    "    df = read_200_lines((csv_file))\n",
    "    if 'A' in csv_file:\n",
    "        global_fracture += len(df) // 100\n",
    "    else:\n",
    "        partial_fracture += len(df) // 100\n",
    "    df[\"RuptureEnergy\"] = (0.5 * df[\"Lnuc\"] * df[\"StrengthDrop\"] ** 2) / (1.158 * mu)  # RuptureEnergy directly introduced\n",
    "    df[\"RuptureEnergy\"] = df[\"RuptureEnergy\"].astype(float)  # Converts the data type to floating point\n",
    "    print(df.shape)\n",
    "    data_frames.append(df)\n",
    "\n",
    "for csv_file in csv_files_100lines:\n",
    "    print(csv_file)\n",
    "    df = read_100_lines((csv_file))\n",
    "    df[\"Logic\"] = 1.0\n",
    "    df[\"StrengthwithNuc\"] = 20.0\n",
    "    df[\"RuptureEnergy\"] = (0.5 * df[\"Lnuc\"] * df[\"StrengthDrop\"] ** 2) / (1.158 * mu)  # RuptureEnergy directly introduced\n",
    "    df[\"RuptureEnergy\"] = df[\"RuptureEnergy\"].astype(float)  # Converts the data type to floating point\n",
    "    print(df.shape)\n",
    "    data_frames.append(df)\n",
    "df = pd.concat(data_frames, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Global fracture num = \" + str(global_fracture))\n",
    "print(\"Partial fracture num = \" + str(partial_fracture))\n",
    "print(df.shape)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6ec164d",
   "metadata": {
    "id": "c6ec164d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, save_path, patience=18, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            save_path : the diectory to store the model\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 18\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        path = os.path.join(self.save_path, 'best_network1.pth')\n",
    "        torch.save(model.state_dict(), path)\t# the parameters of the best model\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9bb8a33d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bb8a33d",
    "outputId": "470f766f-a774-4244-e13b-0ec19a7f726e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Lnuc          X  StrengthDrop  StressDrop  Front      Slip  Logic  \\\n",
      "0  3.388855 -18.822162     35.042084   24.879153  5.848  1.886601    1.0   \n",
      "1  3.388855 -18.352395     35.042084   24.879153  5.760  5.421555    1.0   \n",
      "2  3.388855 -17.882628     35.042084   24.879153  5.664  7.365342    1.0   \n",
      "3  3.388855 -17.412861     35.042084   24.879153  5.600  8.575044    1.0   \n",
      "4  3.388855 -16.943095     35.042084   24.879153  5.504  9.843087    1.0   \n",
      "\n",
      "   StrengthwithNuc  RuptureEnergy  \n",
      "0        24.879153    2080.668269  \n",
      "1        24.879153    2080.668269  \n",
      "2        24.879153    2080.668269  \n",
      "3        24.879153    2080.668269  \n",
      "4        24.879153    2080.668269  \n",
      "       Lnuc          X  StrengthDrop  StressDrop  Front  Slip  Logic  \\\n",
      "0  0.567851  39.196178      26.62081   -2.889506    0.0   0.0    0.0   \n",
      "1  0.567851  38.638309      26.62081   -2.889506    0.0   0.0    0.0   \n",
      "2  0.567851  38.080440      26.62081   -2.889506    0.0   0.0    0.0   \n",
      "3  0.567851  37.522571      26.62081   -2.889506    0.0   0.0    0.0   \n",
      "4  0.567851  36.964702      26.62081   -2.889506    0.0   0.0    0.0   \n",
      "\n",
      "   StrengthwithNuc  RuptureEnergy  \n",
      "0        -2.889506     201.208781  \n",
      "1        -2.889506     201.208781  \n",
      "2        -2.889506     201.208781  \n",
      "3        -2.889506     201.208781  \n",
      "4        -2.889506     201.208781  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "# set random_seed\n",
    "random_seed = 307\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# compute the length of df and set block_size\n",
    "n = len(df)\n",
    "block_size = 100\n",
    "\n",
    "# compute numbers of all small blocks\n",
    "num_blocks = n // block_size\n",
    "\n",
    "# divide the df block by block\n",
    "blocks = [df[i * block_size : (i + 1) * block_size] for i in range(num_blocks)]\n",
    "\n",
    "# randomly shuffle blocks\n",
    "np.random.shuffle(blocks)\n",
    "\n",
    "# concat the blocks into a new df object\n",
    "df_shuffled = pd.concat(blocks, ignore_index=True)\n",
    "df=df_shuffled\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is used to divide the data set\n",
    "def compute_t_times(i, seq_len, prop):\n",
    "    # compute i * prop\n",
    "    target_value = i * prop\n",
    "\n",
    "    # Calculate the nearest integer multiple\n",
    "    nearest_multiple = round(target_value / seq_len) * seq_len\n",
    "\n",
    "    return nearest_multiple\n",
    "\n",
    "total_lines = len(df)\n",
    "print(total_lines)\n",
    "df_for_training1 = df.iloc[0:compute_t_times(total_lines, 100, 0.80)]  # train-set: 80%\n",
    "df_for_validating1 = df.iloc[compute_t_times(total_lines, 100, 0.80):compute_t_times(total_lines, 100,1.0)]  # validation-set: 20%\n",
    "# df_for_testing1=df.iloc[compute_t_times(total_lines,100,1.0):]\n",
    "print(\"Number of training set samples: \" + str(len(df_for_training1) // 100))\n",
    "print(\"Number of validation set samples: \" + str(len(df_for_validating1) // 100))\n",
    "# print(\"Number of testing set samples: \"+str(len(df_for_testing1)//100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4fdba1aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fdba1aa",
    "outputId": "91a3eea2-5f4e-4df2-e6aa-e4d33e3c6360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lnuc maximum: 8.674884   minimum: 0.065104\n",
      "X maximum: 55.34222   minimum: -29.824666\n",
      "StrengthDrop maximum: 98.544479   minimum: 9.064964\n",
      "StressDrop maximum: 41.445763   minimum: -9.749696\n",
      "Front maximum: 26.92   minimum: 0.0\n",
      "Slip maximum: 36.088081   minimum: 0.0\n",
      "Logic maximum: 1.0   minimum: 0.0\n",
      "StrengthwithNuc maximum: 90.087185   minimum: -9.749696\n",
      "RuptureEnergy maximum: 16118.251254452507   minimum: 14.489324170730779\n"
     ]
    }
   ],
   "source": [
    "#Data normalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaler.data_min_ = column_min\n",
    "# scaler.data_max_ = column_max\n",
    "# df_for_training = np.array(df_for_training1)\n",
    "# df_for_validating=np.array(df_for_validating1)\n",
    "df_for_testing=np.array(df_for_testing1)\n",
    "# print(\"train set scale:  \"+str(df_for_training.shape))\n",
    "# print(\"valid set scale:  \"+str(df_for_validating.shape))\n",
    "# print(\"test set scale:  \"+str(df_for_testing.shape))\n",
    "\n",
    "\n",
    "# map each variable to the NN onto the interval [0,1]\n",
    "for i in range(0,9):\n",
    "#     df_for_training[:, i] = (df_for_training[:, i] - column_min[i])/(column_max[i] - column_min[i])\n",
    "#     df_for_validating[:, i] = (df_for_validating[:, i] - column_min[i]) / (column_max[i] - column_min[i])\n",
    "    df_for_testing[:, i] = (df_for_testing[:, i] - column_min[i]) / (column_max[i] - column_min[i])\n",
    "max_Lnuc, min_Lnuc = max(df[\"Lnuc\"]),min(df[\"Lnuc\"])\n",
    "max_X, min_X = max(df[\"X\"]), min(df[\"X\"])\n",
    "max_StrengthDrop, min_StrengthDrop = max(df[\"StrengthDrop\"]), min(df[\"StrengthDrop\"])\n",
    "max_StressDrop, min_StressDrop = max(df[\"StressDrop\"]), min(df[\"StressDrop\"])\n",
    "max_Front, min_Front= max(df[\"Front\"]), min(df[\"Front\"])\n",
    "max_Slip, min_Slip = max(df[\"Slip\"]), min(df[\"Slip\"])\n",
    "max_Logic, min_Logic = max(df[\"Logic\"]),min(df[\"Logic\"])\n",
    "max_StrengthwithNuc, min_StrengthwithNuc =max(df[\"StrengthwithNuc\"]),min(df[\"StrengthwithNuc\"])\n",
    "max_RuptureEnergy, min_RuptureEnergy = max(df[\"RuptureEnergy\"]), min(df[\"RuptureEnergy\"])\n",
    "\n",
    "\n",
    "#Print Maximum and Minimum\n",
    "print(\"Lnuc maximum: \"+str(max_Lnuc)+\"   minimum: \"+str(min_Lnuc))\n",
    "print(\"X maximum: \"+str(max_X)+\"   minimum: \"+str(min_X))\n",
    "print(\"StrengthDrop maximum: \"+str(max_StrengthDrop)+\"   minimum: \"+str(min_StrengthDrop))\n",
    "print(\"StressDrop maximum: \"+str(max_StressDrop)+\"   minimum: \"+str(min_StressDrop))\n",
    "\n",
    "print(\"Front maximum: \"+str(max_Front)+\"   minimum: \"+str(min_Front))\n",
    "print(\"Slip maximum: \"+str(max_Slip)+\"   minimum: \"+str(min_Slip))\n",
    "print(\"Logic maximum: \"+str(max_Logic)+\"   minimum: \"+str(min_Logic))\n",
    "print(\"StrengthwithNuc maximum: \"+str(max_StrengthwithNuc)+\"   minimum: \"+str(min_StrengthwithNuc))\n",
    "print(\"RuptureEnergy maximum: \"+str(max_RuptureEnergy)+\"   minimum: \"+str(min_RuptureEnergy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28j7va6ueXS0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28j7va6ueXS0",
    "outputId": "36d98336-2ab7-4a17-ae41-728f8361cf9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 100, 5) (7550, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "def createXY(dataset,sample_lines):\n",
    "    sample_num=len(dataset)//sample_lines\n",
    "    dataX=[]\n",
    "    dataY=[]\n",
    "    for i in range(0,sample_num):\n",
    "        dataX.append(dataset[(i)*sample_lines:(i+1)*sample_lines,[0, 1, 2, 3, -1]])\n",
    "        dataY.append(dataset[(i)*sample_lines:(i+1)*sample_lines,4:6])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "# trainX2,trainY2=createXY(df_for_training_scaled,block_size)\n",
    "# validateX2,validateY2=createXY(df_for_validating_scaled,block_size)\n",
    "testX,testY=createXY(df_for_testing,block_size)\n",
    "print(testX.shape,testY.shape)\n",
    "# print(trainX2[0,:,:])\n",
    "# print(trainY2[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "JOzB7JcaexnE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOzB7JcaexnE",
    "outputId": "50cc5ad1-a354-48b0-e8c2-1d4a08c4d2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7550\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "# Transform training, validation, and test sets into custom Dataset objects\n",
    "# train_dataset2 = CustomDataset(trainX2, trainY2)\n",
    "# validate_dataset2 = CustomDataset(validateX2, validateY2)\n",
    "test_dataset = CustomDataset(testX, testY)\n",
    "print(len(test_dataset))\n",
    "# Create DataLoader of training sets, validation sets, and test sets\n",
    "batch_size = 256\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "# #trainsets don't need shuffling,already shuffled\n",
    "# validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "G9ltQrGxxXEh",
   "metadata": {
    "id": "G9ltQrGxxXEh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_pro, hidden_dim):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_pro = dropout_pro\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Self-Attention\n",
    "        self.query_projection = nn.Linear(self.embed_dim, self.embed_dim,\n",
    "                                          bias=False)\n",
    "        self.key_projection = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.value_projection = nn.Linear(self.embed_dim, self.embed_dim,\n",
    "                                          bias=False)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads,\n",
    "                                               dropout=self.dropout_pro,\n",
    "                                               batch_first=True)\n",
    "\n",
    "        # Feed Forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self Attention\n",
    "        residual1 = x\n",
    "        transpose = x\n",
    "        query = self.query_projection(transpose)\n",
    "        key = self.key_projection(transpose)\n",
    "        value = self.value_projection(transpose)\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        x = residual1 + attn_output  # Add & Norm\n",
    "        x = self.layer_norm(x)\n",
    "        # Feed Forward\n",
    "        residual2 = x\n",
    "        feed_forward = self.feed_forward(x)\n",
    "        x = feed_forward + residual2  # Add & Norm\n",
    "        x = self.layer_norm(x)  # Layer Norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "372a0ef4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "372a0ef4",
    "outputId": "bdb9a485-ec30-43f3-b0de-f83a209fa3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv1d(5, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv4): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "  (SelfAttention1): TransformerEncoderBlock(\n",
      "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (SelfAttention2): TransformerEncoderBlock(\n",
      "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (SelfAttention3): TransformerEncoderBlock(\n",
      "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (SelfAttention4): TransformerEncoderBlock(\n",
      "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (feed_forward): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (conv7): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv8): Conv1d(256, 2, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "torch.Size([1, 100, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.num_heads = 8\n",
    "        self.embed_dim = 1024\n",
    "        self.dropout_pro = 0.1\n",
    "        self.hidden_dim = 2048\n",
    "        self.conv1 = nn.Conv1d(5, 256, kernel_size=1, stride=1, padding=0) #In conv1d, kernel_size=1, stride=1, padding=0\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(512, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # embeded_dim of Transformer is 1024\n",
    "        self.SelfAttention1 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.SelfAttention2 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.SelfAttention3 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.SelfAttention4 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.SelfAttention5 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.SelfAttention6 = TransformerEncoderBlock(embed_dim=1024, num_heads=8, dropout_pro=0.1,hidden_dim=2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv7 = nn.Conv1d(1024,256,kernel_size=1,stride=1,padding=0)\n",
    "        self.conv8 = nn.Conv1d(256,2,kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN encoder\n",
    "        transpose1 = torch.transpose(x, 1, 2)\n",
    "        conv1 = self.relu(self.conv1(transpose1))\n",
    "        conv2 = self.relu(self.conv2(conv1))\n",
    "        cat1 = torch.cat([conv1, conv2], dim=1)\n",
    "        conv4 = self.relu(self.conv4(cat1))\n",
    "        cat2 = torch.cat([cat1, conv4], dim=1)\n",
    "        transpose2 = torch.transpose(cat2, 1, 2)\n",
    "\n",
    "        # Transformer Processor\n",
    "        #  1th encoder_block\n",
    "        block1 = self.SelfAttention1(transpose2)\n",
    "        # 2nd encoder_block\n",
    "        block2 = self.SelfAttention2(block1)\n",
    "        # 3rd encoder_block\n",
    "        block3 = self.SelfAttention3(block2)\n",
    "        # 4th encoder_block\n",
    "        block4 = self.SelfAttention4(block3)\n",
    "        # 5th encoder_block\n",
    "        block5 = self.SelfAttention5(block4)\n",
    "        # 6th encoder_block\n",
    "        block6 = self.SelfAttention6(block5)\n",
    "\n",
    "        # CNN decoder\n",
    "        x = torch.transpose(block6, 1, 2)\n",
    "        conv7 = self.relu(self.conv7(x))\n",
    "        conv8 = self.conv8(conv7)\n",
    "        Front_Slip = torch.transpose(conv8, 1, 2)\n",
    "        output = self.relu(Front_Slip)\n",
    "        return output\n",
    "\n",
    "\n",
    "# following code is used for testing the output dimensions of the model\n",
    "model = MyModel()\n",
    "\n",
    "print(model)\n",
    "# Define random input data\n",
    "batch_size = 1\n",
    "seq_length = 100\n",
    "input_channels = 5\n",
    "x = torch.randn(batch_size, seq_length, input_channels)\n",
    "\n",
    "# forward propogation and get the output\n",
    "output = model(x)\n",
    "\n",
    "# print the shape of output\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff765526",
   "metadata": {
    "id": "ff765526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv1d(5, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv4): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  (SelfAttention1): TransformerEncoderBlock(\n",
       "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (SelfAttention2): TransformerEncoderBlock(\n",
       "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (SelfAttention3): TransformerEncoderBlock(\n",
       "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (SelfAttention4): TransformerEncoderBlock(\n",
       "    (query_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (key_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (value_projection): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (feed_forward): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (conv7): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv8): Conv1d(256, 2, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define model, Load Model\n",
    "#checkpoint is the path of your model to be tested\n",
    "checkpoint = './results/best_network.pth'\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "state_dict = torch.load(checkpoint)\n",
    "new_state_dict = {}\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    if key.startswith('module.'):  # Assume that the model is wrapped in torch.nn.DataParallel before loading\n",
    "        key = key[7:]  # Remove the 'module.' prefix\n",
    "    new_state_dict[key] = value\n",
    "\n",
    "model = MyModel() # a torch.nn.Module object\n",
    "model.load_state_dict(new_state_dict)\n",
    "print(\"successfully loaded model\")\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "m462Mzc3gBo8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m462Mzc3gBo8",
    "outputId": "aef60361-d1a8-44f5-c09b-fe5451be5960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7550, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "#Predict Front,Slip on TestSet\n",
    "model.eval()  # Switch model to evaluation mode\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "\n",
    "# Bring together all the predictions\n",
    "all_predictions = torch.cat(predictions, dim=0)\n",
    "all_predictions_Tensor = all_predictions.cpu()\n",
    "\n",
    "all_predictions_np = all_predictions_Tensor.numpy()\n",
    "print(all_predictions_np.shape)   # Print the shape of all the predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d81bdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(pred_value, real_value):\n",
    "    # Relative Error defined here\n",
    "    relative_error = 0.0\n",
    " \n",
    "    sum_real = 0.0\n",
    "    added_error  = 0.0\n",
    "    for i in range(block_size):\n",
    "        sum_real += real_value[i, 0]\n",
    "        added_error += abs(real_value[i, 0] - pred_value[i, 0])\n",
    "    if (sum_real == 0):\n",
    "        return 1\n",
    "    relative_error =  added_error / sum_real\n",
    "    return relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc880ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this block shows Front&Slip  This code is used to save svg diagrams and calculate errors\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import matplotlib.pyplot as plt\n",
    "# # visualization\n",
    "block_size = 100\n",
    "\n",
    "iii = 3673\n",
    "pics = 1\n",
    "# Loop here to plot figures\n",
    "for i in range(iii,iii+pics):\n",
    "    #Start with sample_idx\n",
    "    sample_idx=i+seq\n",
    "\n",
    "    # Lnuc\\StrengthDrop\\StressDrop\n",
    "    feature1_actual_front=testY[sample_idx,0:block_size,0] * (column_max[4] - column_min[4]) + column_min[4]\n",
    "    feature1_actual_front = feature1_actual_front.reshape(-1,1)\n",
    "    #print(feature1_actual_front.shape)\n",
    "\n",
    "    feature1_pred_front=all_predictions_np[sample_idx,0:block_size,0] * (column_max[4] - column_min[4]) + column_min[4]\n",
    "    feature1_pred_front = feature1_pred_front.reshape(-1,1)\n",
    "    print(feature1_actual_front.shape)\n",
    "\n",
    "    #restore X\n",
    "    X=testX[sample_idx,0:block_size,1] * (column_max[1] - column_min[1]) + column_min[1]\n",
    "    X=X.reshape(-1,1)\n",
    "    \n",
    "    #print(feature1_actual_front.shape)\n",
    "\n",
    "    #restore Lnuc\n",
    "    Lnuc=testX[sample_idx,0:block_size,0] * (column_max[0] - column_min[0]) + column_min[0]\n",
    "    Lnuc=Lnuc.reshape(-1,1)\n",
    "    #print(feature1_actual_front.shape)\n",
    "\n",
    "    #restore strengthdrop\n",
    "    strengthdrop=testX[sample_idx,0:block_size,2] * (column_max[2] - column_min[2]) + column_min[2]\n",
    "    strengthdrop=strengthdrop.reshape(-1,1)\n",
    "    #print(feature1_actual_front.shape)\n",
    "\n",
    "    #restore stressdrop\n",
    "    stressdrop=testX[sample_idx,0:block_size,3] * (column_max[3] - column_min[3]) + column_min[3]\n",
    "    stressdrop=stressdrop.reshape(-1,1)\n",
    "    #print(feature1_actual_front.shape)\n",
    "\n",
    "    plt.plot(X,Lnuc,label=\"Lnuc\")   #Plot nucleation length\n",
    "    plt.xlabel(\"X*\")\n",
    "    plt.legend()\n",
    "    FaultLength = \"Fault length = \" +str(round(abs(X[0,0] - X[-1,0]), 2 )) + \"km\"\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(X, strengthdrop, label='StrengthDrop') #Plot StressDrop StrengthDrop\n",
    "    plt.plot(X, stressdrop, label='StressDrop')\n",
    "    plt.xlabel('X*')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(X, feature1_actual_front, label='Actual')  #Plot Rupture time(when Rupture front arrives)\n",
    "    plt.plot(X, feature1_pred_front, label='Predicted')\n",
    "    plt.xlabel('X*')\n",
    "    plt.ylabel('Front')\n",
    "    plt.title('Prediction vs Actual - Front')\n",
    "    error_front = 'error='+ str (round(compute_error(feature1_pred_front, feature1_actual_front)*100,4))+\"%\"\n",
    "    plt.text(5, 3, error_front, fontsize=12, fontname='SimHei')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    feature2_actual_slip=testY[sample_idx,0:block_size,1]  * (column_max[5] - column_min[5]) + column_min[5] #Plot Rupture time(when Rupture front arrives)\n",
    "    feature2_actual_slip=feature2_actual_slip.reshape(-1,1)\n",
    "    feature2_pred_slip=all_predictions_np[sample_idx,0:block_size,1]  * (column_max[5] - column_min[5]) + column_min[5]\n",
    "    feature2_pred_slip=feature2_pred_slip.reshape(-1,1)\n",
    "    print(\"Error of slip is \" + str (compute_error(feature2_pred_slip, feature2_actual_slip)))\n",
    "    plt.plot(X, feature2_actual_slip, label='Actual')\n",
    "    plt.plot(X, feature2_pred_slip, label='Predicted')\n",
    "    plt.xlabel('X*')\n",
    "    plt.ylabel('Slip')\n",
    "    plt.title('Prediction vs Actual - Slip')\n",
    "    error_slip = 'error='+ str (round(compute_error(feature2_pred_slip, feature2_actual_slip)*100,4))+\"%\"\n",
    "    plt.text(5, 5, error_slip, fontsize=12, fontname='SimHei')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Error of front is \" + str (compute_error(feature1_pred_front, feature1_actual_front)))    \n",
    "    print(\"Error of slip is \" + str (compute_error(feature2_pred_slip, feature2_actual_slip)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fae969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#Displays MSE and MAE and MSPE metrics for the test set\n",
    "predict_front = all_predictions_np [:,0:block_size,0]\n",
    "predict_slip=all_predictions_np [:,0:block_size,1]\n",
    "actual_front=testY[:,0:block_size,0]\n",
    "actual_slip=testY[:,0:block_size,1]\n",
    "#Print the number of test set samples\n",
    "print(predict_front.shape,actual_front.shape)\n",
    "\n",
    "# MAE\n",
    "print('Rupture time MAE: '+str(metrics.mean_squared_error(actual_front, predict_front)))\n",
    "print('Final slip MAE: '+str(metrics.mean_squared_error(actual_slip, predict_slip)))\n",
    "# MSE\n",
    "print('Rupture time MSE: '+str(metrics.mean_absolute_error(actual_front,  predict_front)))\n",
    "print('Final slip MSE: '+str(metrics.mean_absolute_error(actual_slip,  predict_slip)))\n",
    "# MSPE\n",
    "print('Rupture time MSPE: '+str(np.sqrt(metrics.mean_squared_error(actual_front, predict_front))))\n",
    "print('Final slip MSPE: '+str(np.sqrt(metrics.mean_squared_error(actual_slip, predict_slip))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de352bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Error here\n",
    "import math\n",
    "#Relative Error defined here\n",
    "relative_error_front = 0.0\n",
    "relative_error_slip = 0.0\n",
    "#Dimension Reduction\n",
    "restored_predict_slip = predict_slip * (max_Slip - min_Slip) + min_Slip\n",
    "restored_actual_slip = actual_slip * (max_Slip - min_Slip) + min_Slip\n",
    "restored_predict_front = predict_front * (max_Front - min_Front) + min_Front\n",
    "restored_actual_front = actual_front * (max_Front - min_Front) + min_Front\n",
    "#the number of samples\n",
    "sample_num = all_predictions_np.shape[0]\n",
    "\n",
    "\n",
    "for i in range(sample_num):\n",
    "  single_relative_actual_front = 0.0\n",
    "  single_relative_predict_front = 0.0\n",
    "  single_relative_actual_slip = 0.0\n",
    "  single_relative_predict_slip = 0.0\n",
    "  for j in range(block_size):\n",
    "      single_relative_actual_front += restored_actual_front[i,j]\n",
    "      single_relative_predict_front += restored_predict_front[i,j]\n",
    "      single_relative_actual_slip += restored_actual_slip[i,j]\n",
    "      single_relative_predict_slip += restored_predict_slip[i,j]\n",
    "\n",
    "\n",
    "  relative_error_front += abs((single_relative_actual_front-single_relative_predict_front)/single_relative_actual_front)\n",
    "  relative_error_slip += abs((single_relative_actual_slip-single_relative_predict_slip)/single_relative_actual_slip)\n",
    "relative_error_front/=sample_num\n",
    "relative_error_slip/=sample_num\n",
    "print(\"Rupture Time Relative Error: \"+str(relative_error_front))\n",
    "print(\"Final slip Relative Error: \"+str(relative_error_slip))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
